\documentclass[12pt,twoside]{article}
\usepackage{jmlda}


\begin{document}
\title
    {Оценка оптимального объёма выборки для задач классификации}
\author
    {Харатян~А.\,С., Катруца~А.\,М.$^1$, Стрижов~В.\,В.$^2$} % основной список авторов, выводимый в оглавление
\email
	{haratyan.as@phystech.edu; aleksandr.katrutsa@phystech.edu; strijov@phystech.edu}

\thanks
    {Работа выполнена при финансовой поддержке РФФИ, проект \No\,00-00-00000.
     Научный руководитель:  Стрижов~В.\,В.
     Консультант:  Катруца~А.\,М.}

\organization
    {$^1$Московский физико-технический институт, Москва, Россия;$^2$Вычислительный центр им. А. А. Дородницына ФИЦ ИУ РАН, Москва, Россия}
    
\abstract
	{В статье рассматривается задача выбора оптимального числа объектов выборки для их классификации. Исследуется использование порождающих и разделяющих вероятностных моделей бинарной классификации. Обсуждается проблема медицинской диагностики пациентов. Определяется понятие достаточности объёма выборки. Показывается, какими методами возможно выбрать оптимальное количество объектов, обеспечивающее необходимую точность классификации объектов . В работе рассматривается, применение каких критериев выявляет наилучшее качество классификации. Приводится теоретическое и практическое обоснование предложенных критериев. Используется модель логистической регрессии.

\bigskip
\noindent
\textbf{Ключевые слова}: \emph {определение оптимального объёма выборки, логистическая регрессия, расстояние Кульбака-Лейблера}.

}


\maketitle


\section{Введение}

Работа посвящена оценке оптимального объёма исследуемой выборки применительно к проблемам медицинской диагностики. Рассматриваются биомедицинские данные пациентов как выборка. Каждый пациент набором признаков. 
Получение данных о пациентах требует немалых средст. В случае, если количество данных избыточно, то их измерения приносят крайне неоправданные расходы. В связи с этим поднимается вопрос оптимального количества измерений.
Ввиду дороговизны анализов всех признаков оценка измерений должна быть точной. 

Для нахождения оценки используется модель логистической регрессии\cite{hosmer2013applied}. Стандартной практикой является использование статистических методов\cite{demidenko2007sample} для оценивания объёма данных при помощи логистической регрессии. Введём понятие устойчивости модели в отношении объёма выборки. Будем называть модель устойчивой, если при изменении малом изменении объёма параметры модели меняются незначительно.
Если размер выборки крайне мал и недостаточен, то параметры модели меняются скачкообразно при увеличении объёма.Соответственно, увеличивая объём выборки, мы повышаем устойчивость модели. В качестве показателя устойчивости
для моделей будем использовать расстояние Кульбака-Лейблера. Для того чтобы показать отличие моделей в устойчивости будем использовать разность усредненных значений расстояний К-Л, вычисленных на разных выборках одного и того же объёма. Если объекты порождены одинаковым распределением, то при росте объёма выборок разность расстояний К-Л между моделями падает. Достигнув необходимого показателя устойчивости, можно легко вычислить оптимальный размер данных.

Для вычислительного экперимента используются реальные данные 569 пациентов с 30 признаками и метками об опухоли молочной железы: доброкачественная или злокачественная. Как описано выше, будем обучать модели на разных подвыборках и после достижения необходимого уровня устойчивости получим оптимальный объём данных.


\section{Постановка задачи классификации}

Пусть у нас задана выборка $D = \left\{ \left( \mathbf { x } _ { i } , y _ { i } \right) : i = 1 , \ldots , m \right\}$ с объёмом $m$ объектов(пациентов), каждый из которых описывается $n$ признаками, $\mathbf { x } _ { i } \in \mathbb { R } ^ { n }$ и принадлежит одному из двух классов: $y _ { i } \in \{ 0,1 \}$ . Модель логистической регрессии предполагает, что вектор целевой переменной $\mathbf { y } = \left[ y _ { 1 } , \dots , y _ { m } \right] ^ { T }$ имеет распределение Бернулли, $y _ { i } \sim \mathscr { B } \left( \theta _ { i } \right)$ с плотностью распределения
\begin{equation}\label{eq:frst}
p (  { y } | \boldsymbol { \omega } ) = \prod _ { i = 1 } ^ { m } \theta _ { i } ^ { y _ { i } } \left( 1 - \theta _ { i } \right) ^ { 1 - y _ { i } } 
\end{equation}

Плотность вероятности зависит от вектора параметров $\omega$. Зная $\omega$, можно вычислить вероятность принадлежности к классу 
\begin{equation}\label{eq:scnd}
\theta _ { i } = f \left( \mathbf { x } _ { i } ^ { T } \omega \right) = \frac { 1 } { 1 + \exp \left( - \mathbf { x } _ { i } ^ { T } \omega \right) }
\end{equation}

Пользуясь принципом максимального правдоподобия, можем вычислить функцию ошибки для уравнения \eqref{eq:frst}

\begin{equation}\label{eq:trd}
E ( \boldsymbol { \omega } ) = - \ln p ( \mathbf { y } | \boldsymbol { \omega } ) = - \sum _ { i = 1 } ^ { m } \left( y _ { i } \ln \theta _ { i } + \left( 1 - y _ { i } \right) \ln \left( 1 - \theta _ { i } \right) \right)
\end{equation}

Чтобы найти вектор параметров логистической регрессии $\widehat { \omega }$, необходимо решить оптимизационную задачу:

\begin{equation}\label{eq:frth}
\hat { \mathbf { \omega } } = \arg \min _ { \mathbf { \omega } \in \mathbb { R } ^ { n } } E ( \mathbf { \omega } )
\end{equation}

Тогда алгоритм классификации определяется следующим образом:

\begin{equation}\label{eq:fvth}
a \left( \mathbf { x } , c _ { 0 } \right) = \operatorname { sign } \left( f ( \mathbf { x } , \boldsymbol { \omega } ) - c _ { 0 } \right)
\end{equation}

где $c _ { 0 }$ - пороговое значение функции активации\cite{motrenko2014sample} .

\section{Постановка задачи определения оптимального объёма выборки}
Допустим, что задано $m$ объектов выборки $D$. Количество этих объектов недостаточно для обучения устойчивой модели логистической регрессии. Необходимо, имея $m$ объектов, найти такое число $m^ { * } $, что показатель устойчивости модели при увеличении числа $m^ { * } $ меняется незначительно. Задача нахождения данного числа объектов представляется следующим образом:

\begin{equation}\label{eq:sxth}
m ^ { * } = \min m \in \mathbb { N } \; \;  \forall  k \in \mathbb { N } \rightarrow \rho ( \mathrm { p } ( \boldsymbol { w } | m ^ {*} ) , \mathrm { p } ( \boldsymbol { w } | m ^ {*} + k ) ) < \varepsilon
\end{equation}
где $\rho$ - некоторая функция расстояния между распределениями, $\varepsilon$ - заранее заданный порог устойчивости, $\mathrm { p } ( \boldsymbol { w } | m ^ {*} )$
- распределение весов модели, обученной на выборке размером $m^{*}$.
\bibliographystyle{jmlda-rus}
\bibliography{literature}

\end{document}
\grid
