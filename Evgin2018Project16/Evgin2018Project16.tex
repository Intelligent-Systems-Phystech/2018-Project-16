\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
\usepackage{cite}
\title
    [Оценка оптимального объема выборки для исследований в медицине]
    {Оценка оптимального объема выборки для исследований в медицине}
\author
    {Евгин~А.\,А.$^1$} % основной список авторов, выводимый в оглавление
\thanks
    {Научный руководитель:  Стрижов~В.\,В.
   Задачу поставил:  Катруца~А.\,М.
    Консультант:  T. Гадаев}
\email
    {aleasims@gmail.com}
\organization
     {$^1$Московский физико-технический институт}

\abstract
    {Задача посвящена нахождению оценки оптимальных размеров выборки, необходимых для медицинских иссоедований. Требуется спрогнозировать оптимальные объемы выборки в условиях ограниченного числа измерений. В качетсве алгоритма использутся серия эмпирических алгоритмов оценки объема выборки.

\bigskip
\textbf{Ключевые слова}: \emph {объем выборки}.}

\begin{document}
\maketitle

\section{Введение}
\paragraph{Цель исследования:}
Целью работы является создание и теоретическое обоснование методов оценки объема многофакторных выборок, учитывающих вид модели классификации и более точных по сравнению с известными методами; создание методов классификации малых выборок.
\paragraph{Предмет исследования:}
Оценить минимальный объём выборки — количество производимых измерений некоторого параметра или набора параметров, необходимый для выполнения некоторых ранее сформулированных условий.
\paragraph{Исследуемая проблема:}
Исследование направлено на решение проблемы выбора моделей при классификации выборок малой мощности. По заданной выборке, включающей многокритериальное описание объектов и метки класса объектов, требуется получить оценку структурных параметров, получить асимптотическую оценку необходимого объема выборки и указать предпочтительный подход к решению задачи классификации. Для классификации объекта требуется получить оценку параметров выбранной модели и выполнить анализ ошибок классификации.
\paragraph{Решаемая в данной работе задача:}
В данной работе основное внимание уделяется байесовским методам оценки объёма выборки. Оценка объёма выборки в байесовской постановке включает оценку апостериорного распределения $\mathsf{p}(D|w)$ параметров модели. При отсутствии наблюдаемых данных, апостериорное распределение$$ p(w,D) = \frac{p(D|w)p(w)}{p(D)}$$ совпадает с априорным $\mathsf{p}(w)$, как в классических методах оценки объёма выборки. Разница между вторым и третьим случаями заключается только в способе оценки распределения $\mathsf{p}(w|D)$ — на основе сэмплированных, либо реально наблюдаемых данных.

\paragraph{Предлагаемое решение:}
Статистические методы позволяют оценить объем выборки, исходя из предположений о распределении данных и информации о соответствии наблюдаемых величин предположениям нулевой гипотезы. В случае, если объем исследуемой выборки достаточен или избыточен, возможно применение методов, основанных на наблюдении за изменением некоторой характеристики процедуры построения модели при увеличении объема выборки. В частности, наблюдая за отношением качества прогнозирования на контрольной выборке и обучающей выборке, определим достаточный объем выборки как соответствующий началу переобучения. Таким же образом производится оценка объема выборки в рамках предлагаемого метода: предлагается считать объем выборки достаточным, если расстояние между распределениями, оцененными на подвыборках данного объема, достаточно мало. Такой подход не требует дополнительного обобщения на случай многих переменных. Кроме того, оценку можно производить как при наличии предположений о распределении данных, так и в их отсутствие.
\paragraph{Анализ сильных и слабых сторон предлагаемого решения:}
Недостатком данного подхода является то что количественные оценки возможно получить лишь в случае, когда объем выборки избыточен. В противном случае метод позволяет лишь определить, является ли текущий объем выборки достаточным

\subsection{Постановка задачи}
\paragraph{Постановка задачи классификации:}
Рассмотрим выборку вида $D_m =\{(\mathbf{x}_i, y_i)\}_{i = 1}^{m}$, где $\mathbf{x}_i\in\mathbb{R}^n$~--- описание $i$-го элемента выборки, а $y_i\in \mathcal{Y}$~--- его метка класса. Введем обозначение $D_m = (X,\mathbf{y})$, где $\mathbf{y} =
[y_1,\dots,y_m]^{R}$, $\X = [\mathbf{x}^{R}_1,\dots,\mathbf{x}^{R}_m]^{R}$~--- матрица плана.

В задаче классификации необходимо подстроить отображение наблюдаемых данных $\x \in \mathbb{R}^m$ на множество меток класса $\mathcal{Y}$. Будем называть функцию $a: \mathbb{R}^{m} \rightarrow \mathcal{Y}$ \textit{классификатором}. В данной работе рассматривается подход к построению классификатора, основанными на
максимизации правдоподобия данных.

В данной работе предполагается, если не указано иное, что выборка $(y, X)$ простая, то
есть объекты $(y_i, x_i)$, $i=1, \dots, m$~--- случайные, независимые величины из одного распределения. Будем называть \textit{гипотезой порождения данных} совокупность предположений о
виде неизвестного распределения $\Pgen(y, X)$ элементов выборки $D_m$. В данной работе основное внимание уделяется случаю, когда предположения об истинном распределении данных
формулируется с точностью до некоторого неизвестного параметра $\theta$. Предполагается, что
функция плотности распределения $p(y| X)$ принадлежит параметрическому
множеству функций
\begin{equation*}\label{eq:parametric_family}
\mathcal{F}  = \{f(y, X, \theta), \theta \in \Theta \},
\end{equation*}
то есть найдется такое значение параметра $\theta^{*} \in \Theta$, что
\[\mathcal{P}(y, X) = f(y, X, \theta^{*}). \]

В данных обозначениях предположения о независимости и однородности выборки означают,
что каждая функция $f \in \Fam$ представима в виде произведения
\[f(y, X, \theta) = \prod_{i=1}^{m} g(y_i, x_i, \theta)\]
по элементам выборки.

Пусть фиксировано некоторое значение параметра  $\theta \in \Theta$. Тогда классификатор $a(x)$ имеет вид
\begin{equation*}\label{eq:classifier_mle}
a(x) = \argmax_{y \in \mathcal{Y}} �(y, x, \theta).
\end{equation*}

Таким образом, для решения задачи классификации в данной постановке необходимо
оценить значение параметра $\theta$, наиболее согласованного с наблюдаемыми данными.

\paragraph{Постановка задачи определения оптимального объёма выборки:}
При заведомо недостаточном количестве $m$ объектов выборки $D$ с распределением $\mathsf{P}(\vec{x},y)$ для обучения устойчивой логистической регрессии, требуется найти такое количество $m^*$ объектов выборки, которое будет достаточным и, кроме того никакая другая выборка с количеством объектов $M^*$ большей $m^*$ не приносит новой информации о распределении параметровом логистической регрессии.
 Пусть $\mathbb{P}(\vec{w}|m)$ суть распределение  весов классификатора, который был обучен на выборке $D_m$ с распределением $\mathbb{P}(\vec{x},y)$. Тогда имеем $$ m^* = \min m \in \mathbb{N} ~ \forall k \in \mathbb{N} \rightarrhttps: \rho (\mathsf{P}(\vec{w}|m), \mathsf{P}(\vec{w}|m+k)) < \epsilon$$где $\rho$ ~--- некоторая функция расстояния между распределениями, a $\epsilon$ ~--- заданный порог, в качестве задачи  прогнозирования оптимального объёма выборки. 
\subsection{}
\section{Базовый метод}
Предлагается использовать расстояние Кульбака-Лейблера для определения расстояния между распределениями.
Будем считать распределение векторов параметров нормальным ввиду простоты выборки, а также учитывая факт подчинения целевой переменной Бернулливскому распределению. Найдем набор оптимальных векторов параметров, обучая логистическую регрессию на подвыборках размеров $m-l$ исходной выборки $m$. Для двух подвыборок размера $m$ и $m+k$ из нашего набора имеем
$$\rho (\mathsf{P}(\vec{w}|m), \mathsf{P}(\vec{w}|m+k)) = KL(\mathcal{N}(\vec{\hat w}_1, \boldsymbol{A}_1)||\mathcal{N}(\vec{\hat w}_2, \boldsymbol{A}_2)) =$$
 $$\frac{1}{2}\left( tr(\boldsymbol{A}_2^{-1} \boldsymbol{A}_1) + (\vec{\hat w}_1 - \vec{\hat w}_2)^T \boldsymbol{A}_2^{-1}(\vec{\hat w}_1 - \vec{\hat w}_2) - n + \ln\left(\frac{\det(\boldsymbol{A}_2)}{\det(\boldsymbol{A}_1)}\right) \right),$$
 где $\vec{\hat w_1},\vec{\hat w_2}$ есть матожидания, а $\boldsymbol{A}_1, \boldsymbol{A}_2$ - матрицы ковариации
 векторов параметров их нормального распределения.

Пусть $m^*$ оптимальный объём выборки, по которой распределение $\mathsf{P}(y|\vec{x})$ восстанавливается точно. Малое расстояние $$\rho (\mathsf{P}(\vec{w}|m-l), \mathsf{P}(\vec{w}|m^*))$$ говорит о близости выборки $m-l$ к оптимальному размеру выборки. График зависимости расстояния Кульбака-Лейблера от $m$ достигает своего насыщения при $m = m^*$, ввиду малости изменения $$\rho (\mathsf{P}(\vec{w}|m^* + l), \mathsf{P}(\vec{w}|m^*))$$ при устремлении $l$ к бесконечности. Получим данную зависимость следующим образом. Меняя размер выборки с шагом равным 1, получим $l$ распределений $\mathsf{P}(\vec{w}|m-l)$, а значит и искомую зависимость. Таким образом найдем $m^*$ оптимальное как точку начала насыщения графика.

\subsection{}
\subsection{Вычислительный эксперимент}
\paragraph{Реальные данные:}
Проведем исследование на наборе данных о сердечно-сосудистых заболеваний. Рассматривается 5 признаков на 270 объектах. Рассматриваемая модель - логистическая регрессия. На разных размерах выборки отслеживаются следующие глобальные параметры распределения:

1) логарифмические потери

2) расстояния Кульбака-Лейблера

3) след матрицы ковариации

4) Евклидово расстояние средних

При проекции средних векторов параметров на двумерное пространство наблюдаемое распределение аппроксимируется нормальным.
\paragraph{Синтетические данные:}
Проведем эксперимент на вручную сгенерированых данных с нормальным распределением так, чтобы на них можно было протестировать работу алгоритмов классификации.
Размер синтетических данных соответсвует размеру исходных данных. Проведя эксперимент, было выяснено, что рассматриваемые выше глобальные параметры в модели на реальных данных стабилизируются быстрее, а именно при размерах выборки около 65 объектов. 
\subsection{}
\subsection{Вывод}
Наибольшую ценность из вышеперичисленных параметров составила след матрицы ковариации. Данный компонет выходит на плато при размерах выборок более 80 объектов.
\subsection{}



\bibliographystyle{plain}
\bibliography{Evgin2018Project16}
\nocite{*}
\end{document}
