{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import sklearn\n",
    "import scipy.stats as stats\n",
    "from sklearn.svm import LinearSVC\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "font = {'variant' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 15}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_div_norm(dist1,dist2):\n",
    "    mean1 = np.mean(dist1, axis=0)\n",
    "    cov1 = np.cov(dist1, rowvar=0)\n",
    "    mean2 = np.mean(dist2, axis=0)\n",
    "    cov2 = np.cov(dist2, rowvar=0)\n",
    "    div = 0.5*(np.trace(np.dot(np.linalg.inv(cov2),cov1)) + np.dot(np.dot((mean2-mean1),np.linalg.inv(cov2)),(mean2-mean1)) - cov2.shape[0] + np.log(np.linalg.det(cov2)/np.linalg.det(cov1)))\n",
    "    return div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_distr(X,y,n,count=50):\n",
    "    l = X.shape[0]\n",
    "    C = 1\n",
    "    distr=[]\n",
    "    losses = 0 \n",
    "    clf0 = LogisticRegression(C=C, random_state=0)\n",
    "    clf0.fit(X,y)\n",
    "    for i in range(0,count):\n",
    "        choice = np.random.choice(range(0,l), n)\n",
    "        clf = LogisticRegression(C=C, random_state=0)\n",
    "        while not((0 in y[choice])and(1 in y[choice])):\n",
    "            choice = np.random.choice(range(0,l), n)\n",
    "        clf.fit(X[choice],y[choice])\n",
    "        coef = clf.coef_\n",
    "        distr.append(coef[0])\n",
    "        losses += sklearn.metrics.log_loss(y,clf.predict_proba(X))\n",
    "    return np.asarray(distr), losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"/home/alea/2018-Project-16/Evgin2018Project16/Data.txt\")\n",
    "g = open(\"/home/alea/2018-Project-16/Evgin2018Project16/heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_h = []\n",
    "for line in g:\n",
    "    data_h.append(list(map(float, line.replace(\";\", \" \").replace(\"\\n\", \"\").split())))\n",
    "data_h = np.asarray(data_h)\n",
    "data_h = data_h[:,[0,3,4,7,9,11,13]]\n",
    "np.random.shuffle(data_h)\n",
    "X_h = data_h[:,:-1]\n",
    "X_h = preprocessing.scale(X_h[:,:-1])\n",
    "y_h = data_h[:,-1] - np.ones(data_h[:,-1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_l = []\n",
    "for line in f:\n",
    "    data_l.append(list(map(float, line.replace(\",\",\" \").replace(\"\\n\",\"\").split())))\n",
    "data_l = np.asarray(data_l)\n",
    "np.random.shuffle(data_l)\n",
    "X_l=data_l[:,:-1]\n",
    "X_l = preprocessing.scale(X_l[:,:-1])\n",
    "y_l=data_l[:,-1]-np.ones(data_l[:,-1].shape[0])\n",
    "random_state = 0\n",
    "X, X_t, y, y_t = train_test_split(X_h, y_h, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting and ploting accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1\n",
    "start = 10\n",
    "ratios = []\n",
    "k = y_t.shape[0]\n",
    "accuracy = []\n",
    "for n in range(start, y.shape[0]):\n",
    "    clf = LogisticRegression(C=C, random_state=random_state)\n",
    "    clf.fit(X[:n],y[:n])\n",
    "    y_pred = clf.predict(X[:n])\n",
    "    y_pred_t = clf.predict(X_t[:k])\n",
    "    accuracy.append(np.sum(y_pred_t[:k] == y_t[:k]) / k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy, lw = 5)\n",
    "plt.xlabel('m', fontsize=20)\n",
    "plt.ylabel('Accuracy', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the distribution of regression, and log errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = []\n",
    "losses = []\n",
    "start = 24\n",
    "stop = 100\n",
    "volume = 200\n",
    "for n in range(start, volume-1):\n",
    "    dis, loss = get_param_distr(X[:n],y[:n],n-n//4,count = 300)\n",
    "    distributions.append(dis)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ploting of losses at different sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlist = [(len(distributions)//4)*i for i in range(1,5)]\n",
    "k = 0\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.subplots_adjust(left=0,right =5,bottom = 0, top=5)\n",
    "\n",
    "for m in mlist:\n",
    "    dev=[]\n",
    "    ax = axs[k//2, k%2]\n",
    "    euclid = []\n",
    "    k = k+1\n",
    "    ax.set_autoscale_on(True)\n",
    "    ax.set_ylabel('NLLLoss',fontdict=font)\n",
    "    ax.set_xlabel('m',fontdict=font)\n",
    "    ax.plot(losses[:m],lw=12, color='blue')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploating the Euclidean distance of means depending on the sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.subplots_adjust(left=0,right =5,bottom = 0, top=5)\n",
    "\n",
    "for m in mlist:\n",
    "    dev=[]\n",
    "    ax = axs[k//2, k%2]\n",
    "    euclid = []\n",
    "    for i in range(0,m,2):\n",
    "        w1 = np.mean(distributions[i],0)\n",
    "        w2 = np.mean(distributions[i+1],0)\n",
    "        euclid.append(np.linalg.norm(w1-w2))\n",
    "    k = k+1\n",
    "    ax.set_autoscale_on(True)\n",
    "    ax.set_ylabel('Distance',fontdict=font)\n",
    "    ax.set_xlabel('m',fontdict=font)\n",
    "    ax.plot(euclid[:m],lw=12, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace of coveration matrix depend on sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.subplots_adjust(left=0,right =5,bottom = 0, top=5)\n",
    "\n",
    "for m in mlist:\n",
    "    dev=[]\n",
    "    ax = axs[k//2, k%2]\n",
    "    for i in range(0,m):\n",
    "        dev.append(np.trace(np.cov(distributions[i],rowvar=0)))\n",
    "    k = k+1\n",
    "    ax.set_autoscale_on(True)\n",
    "    ax.set_ylabel('Trace',fontdict=font)\n",
    "    ax.set_xlabel('m',fontdict=font)\n",
    "    ax.plot(dev[:m],lw=12, color='blue') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploating the KL divergention depend on sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.subplots_adjust(left=0,right =5,bottom = 0, top=5)\n",
    "\n",
    "for m in mlist:\n",
    "    divergence=[]\n",
    "    ax = axs[k//2, k%2]\n",
    "    for i in range(0,m):\n",
    "        divergence.append(KL_div_norm(distributions[i],distributions[m]))\n",
    "    k = k+1\n",
    "    ax.set_autoscale_on(True)\n",
    "    ax.set_ylabel('Divergence',fontdict=font)\n",
    "    ax.set_xlabel('m',fontdict=font)\n",
    "    ax.plot(divergence[:m],lw=12, color='blue')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploating the projection of the vector parameters in two-dimensional space depending on the size of the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "fig, axs = plt.subplots(2, 2,sharex=True,sharey=True)\n",
    "fig.subplots_adjust(left=0,right =5,bottom = 0, top=5)\n",
    "for i in range(0,187,50):\n",
    "    fig.suptitle(''+str(i), fontsize=20)\n",
    "    ax = axs[k//2, k%2]\n",
    "    k = k+1\n",
    "    ax.set_autoscale_on(True)\n",
    "    ax.set_title(\"m=\"+str(i))\n",
    "    ax.set_ylabel('y',fontdict=font)\n",
    "    ax.set_xlabel('x',fontdict=font)\n",
    "    ax.scatter(distributions[i][:,2],distributions[i][:,0],500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploating the histogram of distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distributions[70][:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploating the projection of the mean vectors of optimal parameters depending on the sample size on which the regression was studied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "for i in range(0,volume-start-3,1):\n",
    "    means.append(np.mean(distributions[i],axis=0))\n",
    "means = np.asarray(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.subplots_adjust(left=0,right =5,bottom = 0, top=5)\n",
    "\n",
    "for m in mlist:\n",
    "    ax = axs[k//2, k%2]\n",
    "    k = k+1\n",
    "    ax.set_autoscale_on(True)\n",
    "    ax.set_ylabel('y',fontdict=font)\n",
    "    ax.set_xlabel('x',fontdict=font)\n",
    "    ax.scatter(means[:,(k*k+2)%4],means[:,(k+4)%5],300,c = -np.asarray([i for i in range(means.shape[0])]),linewidth=14)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "for i in range(2):\n",
    "    max_size = 2*1000\n",
    "    m_1 = [0,-2]\n",
    "    m_2 = [0,-2]\n",
    "    random_state = 1\n",
    "    a = np.random.multivariate_normal(m_1,np.asarray([[1,0.5],[0.5,1]]), max_size)\n",
    "    b = np.random.multivariate_normal(m_2,np.asarray([[1,-0.5],[-0.5,1]]), max_size)\n",
    "    X1 = []\n",
    "    Y1 = []\n",
    "    data = np.hstack([a,b]).reshape(-1,2)\n",
    "    for i in data:\n",
    "        X1.append(i)\n",
    "        Y1.append(int(i in a))\n",
    "    X1 = np.asarray(X1)\n",
    "    Y1 = np.asarray(Y1)\n",
    "    X.append(np.hstack([X1, Y1.reshape(-1,1)]))\n",
    "Xtrain =X[0]\n",
    "Xtest = X[1]\n",
    "X = Xtrain[:,:-1]\n",
    "y = Xtrain[:,-1]\n",
    "X_t = Xtest[:,:-1]\n",
    "y_t = Xtest[:,-1]\n",
    "plt.scatter(a[:,0],a[:,1])\n",
    "plt.scatter(b[:,0],b[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_fit(inv_pow, x, divergence[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i+1 for i in range(len(divergence)-1)]\n",
    "plt.plot(divergence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
